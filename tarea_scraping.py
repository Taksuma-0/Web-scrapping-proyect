# -*- coding: utf-8 -*-
"""Tarea Scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NgyDTq1f4tODGbSiExAdVnYLKPCh4H32

# Autores

- Welinton Barrera
- Joaquin Araya

## Actividades

### 1. Conexión a la API de Reddit

- Crear una aplicación en [Reddit Apps](https://www.reddit.com/prefs/apps/) para obtener tus credenciales.
- Usar la librería `PRAW` para conectarte a Reddit mediante un script en Python.

### 2. Extracción de datos

- Elegir entre **3 y 5 subreddits** para analizar.
- Justificar la elección de los subreddits en una celda markdown.
- Obtener información de al menos **100 publicaciones por subreddit**. Para cada post, guardar al menos:

  - ID del post  
  - Título  
  - Fecha de creación  
  - Autor  
  - URL  
  - Número de comentarios  
  - Texto del post (si tiene)  
  - Puntaje (score)  
  - Cantidad de upvotes / downvotes (si es posible)

- Extraer al menos **10 comentarios por post**, incluyendo:

  - ID del post
  - ID del comentario  
  - Autor  
  - Texto  
  - Fecha

### 3. Estructura del resultado

- Organizar la información en al menos **dos DataFrames**:
  - Uno para los **posts**
  - Uno para los **comentarios**

- Exportar ambos datasets en formato `.csv` o similar.
'''
"""

!pip install praw pandas duckdb

import requests
from bs4 import BeautifulSoup
import pandas as pd
import os
import praw
import datetime
import duckdb

"""Función para obtener lo deseado de reddit, esto utilizando las credenciales anteriores."""

reddit = praw.Reddit(
    client_id= "9mdmbV0o4tskHsXk5NHxQQ",
    client_secret = "P3O7kKvsll_DQiEkj2LzXKVl_yO5mA",
    user_agent = "taksuma",
    check_for_async=False
) ## Aqui se ingresa nuestra credenciales creadas anteriormente en https://www.reddit.com/prefs/apps/, pues aqui cremos la api para hacer la coneccion.
#Credenciales joaquin:
#(client_id= "Rlp9XfwzfYgcoP-9l_VDsA",
#user_agent = "BedroomNeither783",
#client_secret = "2thBkidXIrNlio7JC_fTYY06iGqKXg")

def info_subreddit(subreddit):
  print(f"Nombre del subreddit r/{subreddit.display_name}")
  print(f"Numero de subscriptores del subreddit: {subreddit.subscribers}")
  print(f"URL del subreddit: {subreddit.url}")
  print("---------------------------------------------------------------------------")
  print(f"Descripcion del subreddit: {subreddit.public_description}")

datos = {
    "id_post":[],
    "titulo_post":[],
    "fecha_post":[],
    "autor":[],
    "URL":[],
    "N_comentarios":[],
    "tiene_text":[],
    "texto_post":[],
    "puntaje":[],
    "upvotes":[],
    "downvotes":[]

}

comentario = {
    "id_post":[],
    "id_comentario":[],
    "autor_comentario":[],
    "texto_comentario":[],
    "fecha_comentario":[]

}

def extraccion_info_post(post):
    fecha = datetime.datetime.fromtimestamp(post.created_utc).strftime('%Y-%m-%d %H:%M:%S')
    autor = str(post.author) if post.author else "ELIMINADO"
    tiene_texto = "Sí" if post.selftext else "No"

    comentarios_cantidad = post.num_comments ## Numero de comentarios

    if (comentarios_cantidad>=10):
        datos['id_post'].append(post.id)
        datos['titulo_post'].append(post.title)
        datos['fecha_post'].append(fecha)
        datos['autor'].append(autor)
        datos['URL'].append(post.url)
        datos['N_comentarios'].append(post.num_comments)
        datos['texto_post'].append(post.selftext)
        datos['tiene_text'].append(tiene_texto)
        datos['puntaje'].append(post.score)
        datos['upvotes'].append(post.ups)
        datos['downvotes'].append(post.downs)


        post.comments.replace_more(limit=0)  # Evita cargar "load more comments"
        for comment in post.comments[:10]:
                  if not comment.stickied:  # Ignorar comentarios fijados (usualmente son de moderadores)
                      comentario['id_post'].append(post.id)
                      comentario['id_comentario'].append(comment.id)
                      comentario['autor_comentario'].append(str(comment.author) if comment.author else "ELIMINADO")
                      comentario['texto_comentario'].append(comment.body)
                      comentario['fecha_comentario'].append(
                          datetime.datetime.fromtimestamp(comment.created_utc).strftime('%Y-%m-%d %H:%M:%S')
                    )

"""- Primer subreddit

Se eligió el subreddit r/chileIT para la tarea de web scraping debido a que es una comunidad activa y especializada en temas de tecnología de la información en Chile, lo que garantiza datos relevantes, variados y con un enfoque temático claro en lo que es la comunidad tecnologica chilena. Al extraer los 100 posts más destacados (sub.top) del último año, se obtiene contenido reciente y valorado por la comunidad, ideal para análisis de texto, minería de opiniones o estudios sobre tendencias tecnológicas en el contexto chileno.
"""

subs=reddit.subreddit("chileIT")

for submission in subs.top(limit=100, time_filter='year'):
    extraccion_info_post(submission)

df_posts = pd.DataFrame(datos)
df_comentarios = pd.DataFrame(comentario)

df_posts

df_comentarios

"""# Pasamos a exportar los datos a un csv"""

df_comentarios.to_csv('comentarios.csv', index=False)
df_posts.to_csv('posts.csv', index=False)

"""- Segundo subreddit

Se eligió el subreddit r/ciberseguridad para realizar la tarea de web scraping porque es una comunidad activa y especializada donde se comparten temas clave en el área de seguridad informática, como vulnerabilidades recientes (por ejemplo, CVE), herramientas como Wireshark o Burp Suite, y experiencias relacionadas con hacking ético. Al extraer los 100 posts más destacados del último año, se obtiene un conjunto de datos variado y actualizado, ideal para aplicar técnicas de análisis de texto y entender mejor las preocupaciones, tendencias y tecnologías más discutidas en el ámbito de la ciberseguridad, desde una perspectiva real y aplicada.
"""

datos = {
    "id_post":[],
    "titulo_post":[],
    "fecha_post":[],
    "autor":[],
    "URL":[],
    "N_comentarios":[],
    "tiene_text":[],
    "texto_post":[],
    "puntaje":[],
    "upvotes":[],
    "downvotes":[]

}

comentario = {
    "id_post":[],
    "id_comentario":[],
    "autor_comentario":[],
    "texto_comentario":[],
    "fecha_comentario":[]

}

sub=reddit.subreddit("ciberseguridad")

for submission in sub.top(limit=1000, time_filter='year'):
    extraccion_info_post(submission)

df_posts2 = pd.DataFrame(datos)
df_comentarios2 = pd.DataFrame(comentario)

df_posts2

df_comentarios2

"""# Pasamos a exportar los datos a un csv"""

df_comentarios2.to_csv('comentarios2.csv', index=False)
df_posts2.to_csv('posts2.csv', index=False)

"""- Tercer subreddit

Se eligió el subreddit r/programacion para esta tarea de web scraping porque es una comunidad hispanohablante muy activa donde se discuten temas relacionados con el desarrollo de software, aprendizaje de lenguajes, y experiencias laborales en el área. Al extraer los 100 posts más populares del último año, es posible analizar tendencias sobre qué lenguajes de programación están siendo más recomendados o utilizados (como Python, JavaScript o Rust), además de identificar recursos de aprendizaje, dudas frecuentes de principiantes, y tecnologías emergentes. Esto permite construir un dataset útil para estudiar el interés actual en herramientas, frameworks y buenas prácticas dentro de la comunidad de habla hispana.
"""

datos = {
    "id_post":[],
    "titulo_post":[],
    "fecha_post":[],
    "autor":[],
    "URL":[],
    "N_comentarios":[],
    "tiene_text":[],
    "texto_post":[],
    "puntaje":[],
    "upvotes":[],
    "downvotes":[]

}

comentario = {
    "id_post":[],
    "id_comentario":[],
    "autor_comentario":[],
    "texto_comentario":[],
    "fecha_comentario":[]

}

sub=reddit.subreddit("programacion")

for submission in sub.top(limit=100, time_filter='year'):
    extraccion_info_post(submission)

df_posts3 = pd.DataFrame(datos)
df_comentarios3 = pd.DataFrame(comentario)

df_posts3

df_comentarios3

"""# Pasamos a exportar los datos a un csv"""

df_comentarios2.to_csv('comentarios3.csv', index=False)
df_posts2.to_csv('posts3.csv', index=False)